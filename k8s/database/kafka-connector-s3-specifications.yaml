apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  name: s3-sink-specifications
  namespace: apc-striming-ns
  labels:
    strimzi.io/cluster: kafka-connect-cluster
spec:
  class: io.confluent.connect.s3.S3SinkConnector
  tasksMax: 3
  config:
    # Kafka 설정
    topics: danawa-crawl-specifications
    topics.regex: ""
    topics.dir: "danawa-crawl-specifications"
    
    # S3 설정
    s3.bucket.name: "yaml-382045063773"  # Account ID는 실제 값으로 변경 필요
    s3.region: "us-east-1"
    s3.part.size: 5242880  # 5MB
    flush.size: "1000"
    rotate.interval.ms: "3600000"  # 1시간
    rotate.schedule.interval.ms: ""
    
    # 파일 형식 설정
    format.class: io.confluent.connect.s3.format.json.JsonFormat
    partitioner.class: io.confluent.connect.storage.partitioner.TimeBasedPartitioner
    partition.duration.ms: "3600000"  # 1시간
    path.format: "YYYY/MM/dd/HH"
    locale: "ko"
    timezone: "Asia/Seoul"
    
    # 스키마 설정
    schema.compatibility: NONE
    value.converter: org.apache.kafka.connect.json.JsonConverter
    value.converter.schemas.enable: false
    
    # S3 접두사
    s3.compression.type: none
    store.url: "s3://yaml-382045063773/crawler-data/danawa-crawl-specifications"
    
    # 에러 처리
    errors.tolerance: all
    errors.log.enable: true
    errors.log.include.messages: true

